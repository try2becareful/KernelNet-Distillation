## KernelNet-Distillation

# Основная идея
Вместо того, чтобы назначать индивидуальный параметр свободного веса между каждым входным и выходным нейроном слоя нейронной сети, следует связать с каждым входом и выходом вектор свободных параметров, который мы подразумеваем как местоположение в низко-размерном пространстве. Затем вес между двумя единицами устанавливается как некоторая фиксированная функция расстояния между этими местоположениями (или, точнее, взвешенная ядерная функция векторов местоположения). Возможный выбор ядра будет заключаться в том, что нейроны, которые находятся далеко друг от друга, не связаны между собой, а нейроны, которые находятся близко друг к другу, сильно связаны. Таким образом, функция слоя состоит в том, чтобы создать гладкую функцию ядра, центрированную во входных точках, которая выбирается в нескольких точках выходными нейронами (а именно в их местоположениях). 
Рекомендательные системы обычно работают с разреженными данными высокой размерности. Например, задача состоит в том, чтобы предсказать рейтинг фильма для пользователя на основе данных миллионов других пользователей, каждый из которых видел небольшое подмножество из тысяч фильмов. В таких ситуациях принято считать, что редко наблюдаемые записи матрицы, на основе которых нужно сделать обобщение, могут быть каким-то образом представлены в пространстве низкой размерности. Действительно, рейтинги фильмов предположительно коррелируют с относительно небольшим числом характеристик: например, с комбинацией актера, играющего в фильме, и жанра фильма. Поскольку ожидается, что данные будут лучше всего объясняться такой низко-размерной моделью, кажется, что это хорошо подходящая среда для KernelNet.

# Дистилляция
Дистилляция - это такая техника, когда берется некая модель, обговаривается, что она будет teacher моделью и из нее будет происходить процесс дистилляции - получения student модели. Наша задача, чтобы предсказания student модели были как можно ближе к предсказаниям teacher модели, а сама по себе student модель была в несколько раз меньше. Мы обучаем student не только на исходную задачу, на которую обучалась teacher модель, но и обучаем ее с учетом предсказаний teacher модели. 

# Результат
Сравниваем модели X и Z:  
Несмотря на то, что rmse во время обучения модели Z немного превышает значения rmse во время обучения модели X, модель Z значительно меньше в своих размерах, а так быстрее получает результат. Данные преимущества модели Z затмевают недостаток увеличения rmse. Благодаря дистилляции мы добились выигрыша в памяти и скорости, что значительно уменьшает вычислительную сложность модели. Модель использовать довольно валидно, так как rmse ухудшилось приблизительно на 15%, но при этом размер модели уменьшился в 5 раз. Данная модель могла бы успешно применяться непосредственно при работе с рекомендательными системами, что позволит избавиться от некоторых затрат.
